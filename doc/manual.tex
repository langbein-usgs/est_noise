\documentclass[12pt]{amsart}
%\documentclass[12pt,oneside,letter]{article}
%\documentclass[12pt]{article}

\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{Manual for est\_noise7.3x}
\author{John Langbein \\
US Geological Survey \\
Menlo Park, CA \\
langbein@usgs.gov}
%\date{July 14, 2016} 
%\date{May 15, 2021}   
\date{Sept 20, 2022}                                        % Activate to display a given date or no date

\begin{document}
\maketitle
\section{Introduction}

The program \textit{est\_noise7.3x} does time-series analysis to extract trends (velocities), offsets, and other
parameters that characterize GNSS observations (or any other time series) and simultaneously estimate the parameters the characterize the background noise
of the data. Basically, it does least-squares regression, but importantly, it quantifies the temporal correlations of the data to
provide unbiased estimates of the parameters that model the time-dependence of the data and their statistical errors. Standard least-squares regression typically assumes that each observation is independent, and drawn from a Gaussian distribution. However, for many time series, the data are temporally correlated.  \textit{est\_noise7.3x} simultaneously "measures" the temporal correlations and estimates the usual parameters that one often seeks from a time series.
The optimization is measured using the maximum likelihood estimator.  Although this code was written primarily for
GNSS observations of positions, it can be applied to other time series, too.

\textit{est\_noise7.30} is an update of my older program, \textit{est\_noise6.50} and fixes a problem found in \textit{est\_noise7.22}.  There are several improvements:
\begin{enumerate}
\item {It provides a mechanism to analyze large data sets rapidly. For data with no gaps in the time series, the speed-up
can be up to a factor of 50. Typically, for many GPS time series, which have a few gaps, the speed-up is about a factor of 8.}
\item {It can take a variety of time formats including modified julian day and GMT style formats}
\item {The data and ancillary matrices are now double precision which minimizes apparent singularities in the inversion of data
covariance matrices having large temporal correlations.}
\item {Since the input or configuration to this program is a series of answers to questions, the program provides a
\textit{journal} file of the responses.  One can edit and then reuse the \textit{journal} file.}
\item {All subroutines are public domain. The legacy \textit{est\_noise6.50} used some subroutines from \textit{Numerical Recipes, Press et al.}. Those included a Fourier Transform, and the  \textit{Nelder and Mead} (1965), Downhill simplex optimization codes.
I have written my own version of Nelder and Mead and used the public domain FFT from netlib.org}
\item {The same set of inputs for \textit{est\_noise6.50} can be used with \textit{est\_noise7.3x} which yields the same
results; that is  \textit{est\_noise7.3x} is backwards compatible with  \textit{est\_noise6.50}.}
\end{enumerate}

This current version, 7.30, is an incremental update to version 7.22.  The primary change is related to the formation of the
covariance matrix that characterized Gauss-Markov noise. The new version better accounts for the time period prior to the start
of the time series. Neglecting this caused the old version to underestimate the rate uncertainty and biased the estimate of rate.
For other noise models, this has not been a problem.

Included in the top directory for \textit{est\_noise} is a file called \texttt{BugFix.txt} which provides the changes in version 7 of  \textit{est\_noise} in
chronological order.  There have been numerous enhancements, mostly to the scripts (found in the \texttt{example0} directory)
that can be used to "drive" \textit{est\_noise}.

The development and discussion of \textit{est\_noise} can be found in the references. 

\subsection{Time dependent models}

The following types of time dependence are included with \textit{est\_noise};  rate, changes in rate, offsets, sinusoids,
exponential, $1-e^{-t/{\tau}}$, and Omori decay, $log(1 + t/{\tau})$. In addition, more functions can be added.  For instance,
some observations of interest, such a borehole strainmeter data, one might include the dependence of another set of observations such as atmospheric pressure. The pressure data can be used as another time-dependent function and the
program finds the admittance (or gain) of the pressure upon the strain data. Specifying these functions is done by answering
questions that the program asks. The times of the offsets, the time of the start of the exponential trend,
and the period of time spanning the rate change need to be provided as part of the input.  
More information will be provided later in the manual.

\subsection{Noise models for temporal correlations}

The temporal correlations can be modeled in terms of a power spectra, $P(f)$, where $f$ is frequency. \textit{est\_noise}
has a library of a few, simple noise models which can replicate a variety of temporal correlations. These include,
white noise, where $P(f)={\sigma^2}/f_{ny}$ where $f_{ny}$ is the Nyquist frequency and $\sigma$ is the amplitude
of white noise.  This is represents, uncorrelated observations, and that the data are Normally distributed with a standard
deviation of  $\sigma$.

However, most data sets have temporal correlations that can be represented by a power-law power-spectrum, $P(f)=P_o/f^{\beta}$. If $\beta =1$, then this is termed flicker noise, and if $\beta =2$, then this is termed random-walk noise.

In addition, \textit{Langbein} (2004) provided two more types of noise; generalized Gauss-Markov and bandpass filtered noise.
Generalized Gauss Markov noise can be represented by
\begin{equation}
P(f)~= ~{{P_o} \over [f^2 + ({\alpha}/{2{\pi}})^2]^{{\beta}/2}}
\end{equation}
where the power spectra is frequency independent at the lowest frequencies, but becomes a power law when $f > {\alpha}/{2{\pi}}$. (Note: this corrects equation~15 in \textit{Langbein} (2004)). When $\beta=2$, this is known as first-order Gauss-Markov (FOGM) noise.

For band pass filtered noise, it is represented by:
\begin{equation}
P(f)~= ~{{\sigma_{bp}}^2  [ {{(f/f_h)^2} \over {(1 + (f/f_l)^2)(1 + (f/f_h)^2)} }] ^{2p}}
\end{equation}
where $f_l$ and $f_h$ are the high and low frequency limits of the filter and $p$ is the number of poles.

All of these noise sources can be added together to provide a more complex and realistic representation of the temporal correlations.  As \textit{Langbein} (2017) demonstrates, there are two different ways to add these noise sources and
that can impact the computational requirements.  It customary to assume that each noise source is independent
and add them in quadrature, which has been done with the legacy program \textit{est\_noise6.50} and is offered
as one option in \textit{est\_noise7.3x}.  Alternatively, each of these noise models is represented by
a time-domain equivalent, or filter functions.  These filters can be added together to form the covariance
matrix. Computationally, this offers some advantages that are incorporated in \textit{est\_noise7.3x}.

\subsection{Related programs}

The tar-ball contains several other programs that augment \textit{est\_noise} which can help test the code and/or help evaluate the results.
\begin{description}
\item[bust\_5]  This removes outliers from time series based upon a running median
\item[compare\_wander7]  Computes a spectrum of \textit{wander} or drift based upon \textit{Agnew} [1992]. This is
useful to assess the quality of the estimated noise model obtained from \textit{est\_noise}.  
\item[psd\_calc7] Computes the equivalent power spectral density using the noise model consistent with \textit{est\_noise}.
This involves re-scaling the coefficients.
\item[gen\_noise7] This program generates a time series of simulated, colored noise using the noise models discussed
above.
\item[adjust\_1]  removes sinusoids, offsets, rate, changes in rate, and exponential/Omori decay when given their
specified amplitudes. This is used in companion with \textit{bust\_5}.
\end{description}

In the \texttt{example0} directory, I provide 5 scripts that combine the use of all of these programs to remove outliers
and to perform the time series analysis to extract trends and the background noise of the data. 

Finally, \textit{Bos et al.} (2012) provides an alternative to \textit{est\_noise} and that code can be found
at
\\
\texttt{http://segal.ubi.pt/hector/}
\\
\textit{est\_noise} primary advantage is that it has no approximations.  For power-law noise with indices greater than $1$,
\textit{hector} makes an approximation to force the covariance matrix to be Toeplitz and uses a fast Toeplitz solver
to invert the covariance matrix. \textit{est\_noise}, when using the simple addition of filter functions provides similar computational
speed to \textit{hector} without the approximation. In reality, for \textit{hector}, approximation has relatively minor influence on
its results based upon tests between the two programs.


\section{Compiling the program}

The program is written in Fortran, primarily the legacy Fortran 77. Unlike the legacy \textit{est\_noise6.50}, I'm not
able to provide an executable file. Instead, you will need to compile the program and its subroutines. I've successfully
compiled the program on Mac OS and Linux OS, using both gfortran and the Intel Fortran compilers. 
I have not tried compiling and running the program on Windows OS. The best performance
comes with the Intel compiler and its MKL libraries.  By using gfortran and the associated openblas library, the computation speed is reduced by approximately 30\%.  For Linux and gfortran, you may need to download the openblas library.


I have provided a \textit{compile} script; to invoke,
\\
\texttt{./compile.sh}
\\
For which:
\\
\texttt{Script used to compile est\_noise \\
Usage:  compile.sh mac/linux i/g  \\
  where argument one designates the OS, mac or linux  \\
    and argument two designates the compiler \\
    i   intel ifort  \\
    g   gfortran
}

For compiling on a Mac, you will need Apple's Xcode installed which is obtained via Apple's online "App" store.
With the Intel option, \textit{i},
this requires Intel Fortran and their Math Kernal Library.  Otherwise, you'll need \textit{gfortran} which can
be obtained at \texttt{http://hpc.sourceforge.net/} or via \texttt{brew install gfortran}, assuming that the \texttt{brew} package
manager has been installed. Likewise, \textit{openblas} can be obtained with the command  \texttt{brew install openblas}.

For compiling on Linux either Intel Fortran or \textit{gfortran} will work.  If the Intel compiler with MKL
is not available, then gfortran is easily obtained (if not already installed).  Depending on the Linux
distribution, a package installer, such as \texttt{yum}, will install gfortran.

In addition, if your are using gfortran on linux, the openblas library needs to be installed either in  \texttt{/usr/lib} or \texttt{/usr/lib64} and the listing will look like:
\\
\tiny{
\texttt{
ls -lt /usr/lib/*openblas*
\\
lrwxrwxrwx 1 root root       22 Jan 28  2015 /usr/lib/libopenblas.so.0 -> libopenblas-r0.2.11.so
\\
-rwxr-xr-x 1 root root 15134496 Aug 24  2014 /usr/lib/libopenblas-r0.2.11.so
}
}
\normalsize{ }
\\
If not present, you can simply download the openblas libraries using a package installer such as \texttt{yum}, that
is \texttt{sudo yum install openblas}, which is used with the CentOS dialect of Linux. IN ADDITION, you will need to
\textit{comment-out} one of two lines in compile.sh script near line 40.

Note for ubuntu OS:  For this OS, I downloaded \textit{openblas} from 
\\
\texttt{http://www.openblas.net/} (look for download tab near top
of page) and follow the install instructions 
\\
\texttt{https://github.com/xianyi/OpenBLAS/wiki/Installation-Guide}
\\
using \texttt{make}.
You will need to run \texttt{make} a few times with different arguments but those are provided by the output of the previous \texttt{make} command.  The \texttt{apt-get install} may work, too.

\textit{A word of apology:}  Unfortunately, compiling \textit{est\_noise} is some-what a "crapshoot".  Depending on the
versions of compilers and the versions of your OS, you may need to change the various compiler options to be able
to not only successfully compile \textit{est\_noise}, but to get it to run, too. In the \texttt{compile.sh} script, you will
see a number of "commented-out" compile statements; these might provide you some hints about what you might
need to change to get the program(s) to work. Recently, for gfortran, I had to add an additional option of \texttt{-std=legacy}
to the compilation since the FFT subroutines are written either for Fortran 77 or Fortran 66. Finally, as of this writing, Sept. 2022, 
I have not tested the Intel compile as over the past few years, Intel has made changes with the distribution of their software.

\section{Initial test}

An example is included with the distribution. Critically, it contains the seed file, \texttt{seed.dat}, the data \texttt{canp.e}, and the
input driver file, \texttt{est\_canp.in}. These are found in the \texttt{example} directory. To run,
\\
\texttt{../bin/est\_noise7.30 < est\_canp.in} \\
On a single core, 64 bit Linux computer running 2.2GHz, the program take ~48 seconds when compiled with the Intel Fortran and ~68 seconds with gfortran.

Although the program can run over multiple cores, I find that it is most efficient to use a single core.  That is, if you are 
analyzing several time series, one should assign a single process of \textit{est\_noise} to run on a single core processing
a single time series. That way, if you have 8 core available, one can be simultaneously analyzing 8 different time series. However, one needs to
create a directory structure for each time series. The Linux utility, \texttt{taskset}, assigns a process to specific cores. For example,
\\
\texttt{taskset -c 0 est\_noise7.30 < est\_canp.in}\\
assigns the process to the first core.  Unfortunately, this utility may not available on Macs. (Note,  \texttt{brew install util-linux} may
include \texttt{taskse}t)

The input file, \texttt{est\_canp.in}, contains some annotations about the meaning of each line.  You can compare
the standard out (aka screen dump) with \texttt{est\_canp.out}.  The most critical information is found at the tail
end of the output which includes the estimate of rates, offset, log-trends, sinusoids, and the noise model.  More information
is contained in the following sections.

\section{Input files}

At the minimum, \textit{est\_noise} requires two input files, one \texttt{seed.dat}, which is a single integer number
to act as a seed for a random number generator, and a file of data.  When the program is run, it will start
asking questions on the standard output and responses are required to be typed into the
standard input. The answers are recorded in a journal file which
can be re-used after changing its name. This section is broken into two parts, the first part discusses formats, in particular
for the data and specifying time, and the second part documents the questions and answers required by
\textit{est\_noise}

\subsection{Data formats}

\textit{est\_noise} handles five different formats for time.  Within the program, the basic unit is day.  The time formats
are specified by:
\begin{itemize}
\item \texttt{otr} Time is specified as year, and day of year ranging from 1 to 365 (or 366 for leap year). Finer resolution
is specified as fraction of a day.  For instance, 1515 and 10 seconds on 1, December 2015 is \texttt{2015 335.635532407}. These are read and stored as double precision values. This format has been well tested.
\item \texttt{otd} Time is specified as year, month, and day of month.  Month is a number from 1 to 12. For 
1515 and 10 seconds on 1, December 2015, it is specified as \texttt{20151201.635532407}
\item \texttt{otx} Time is specified as year, month, and day of month.  Month is a number from 1 to 12. For 
1515 and 10 seconds on 1, December 2015, it is specified as \texttt{2015 12 1.635532407}
\item \texttt{gmt} This uses the time format consistent with the GMT plotting package, \textit{www.soest.hawaii.edu/gmt/}.
Therefore, 1515 and 10 seconds on 1, December 2015, it is specified as \texttt{2015-12-01T15:15:10.0}
\item \texttt{mjd} Time is specified as modified Julian day; Therefore, 1515 and 10 seconds on 1, December 2015, it is specified as \texttt{57357.635532407}. Day zero is 17, November, 1858.
\end{itemize}

The time series of data requires a time stamp, using one of 5 formats, the observation, followed by its "error". Actually,
that last column, although required by the \texttt{read} statement, is not used. Some thought is needed with
regards to the data-value.  Although the program is written for double precision arithmetic, the Fortran format statements
provide limited resolution. For instance, if the actual scale of the variations in the observations is a few
millimeters, then it is best to scale the data to tmillimeters and not meters. 


\subsection{\textit{est\_noise} questions and responses}

The question/answers are generally grouped into two parts, one that specifies the components of the time-dependent functions and the second part that specifies the components of the noise model.  The time dependent model is the summation of a rate, changes in rate, offsets, sinusoids, exponentials, $1 - e^{-t/{\tau}}$, logarithmic trends,
$log(1 + {t/{\tau}})$, and auxiliary functions or data, $a(t)$.


\begin{description}
\item[Format of input]  Provide the format of dates used to specify the limits of the data and the time dependent model; use one of the five abbreviations listed in \textit{Data formats}. 

\item[Number of time series] This will usually be \texttt{1}, but in special cases, such as legacy electronic distance meter
data where near simultaneous measurements were made to neighboring monuments, this value could be greater than \texttt{1}. This option adds additional columns in the design matrix having either $0$ or $1$ depending upon whether
the measurement was made to monument 1 or 2...

\item[Time interval] Specify the time interval to analyze the data. Initial date followed by ending date. The dates must be
consistent with the date format descriptor specified in the first answer. It is possible that your data set spans a longer
time interval than you wish to analyze. The input provides a means to limit the interval for analysis. In addition, the
\textbf{start time has a special meaning} as it provides the reference time.  For instance, when fitting a secular trend to data, $d_i = B + R (t_i -t_{0})$, the nominal value $B$ represents the estimate of $d$ at the \textbf{start time}, $t_{0}$.

\item[Secular rate, y/n]  Estimate the secular rate, \textit{yes or no}.  That is, estimate $R$ in $d_i = B + R (t_i -t_{0})$.
If \texttt{n}, then $R=0$.

\item[Number of rate changes]  Normally \texttt{0}, but if a rate change is specified, then the number of rate changes is specified.  Following this entry, if not equal to \texttt{0}, are \texttt{n} lines that specify each interval of the rate change, both the start and end date of the change using the same time format entered above.  Each rate change interval must be
within the specified interval of the data given in \textbf{Time interval}. The $k^{th}$ rate change, $r_k$ over the interval
$t_{k1}$ to $t_{k2}$ is:
\begin{equation}
\begin{aligned}
d_i & = B + R (t_i - t_0)  & \text{ for } t_i < t_{k1} \\
      & = B + R (t_i - t_0) + r_k (t_i - t_{k1}) & \text{ for } t_{k1} \leq t_i \leq t_{k2} \\
      & = B + R (t_i - t_0) + r_k (t_{k2} - t_{k1}) & \text{ for } t_i >  t_{k2}
\end{aligned}
\end{equation}
None of the time intervals for the rate changes should fall outside the limits specified in \textit{Time interval} discussed above.

\item[Number of sinusoids] Input the number of sinusoids. If the number is more than \texttt{0}, then list the
period of each sinusoid in \textbf{days}. The $k^{th}$ sinusoid is the sum of the sine and cosine;
\begin{equation}
d_i  = B + R (t_i - t_0) + C_k \cos (2 {\pi}(t_i - t_0)/{p_k})+ S_k \sin (2 {\pi}(t_i - t_0)/{p_k})
\end{equation}
where $p_k$ is the period of the $k^{th}$ sinusoid with amplitudes $C_k$ and $S_k$.  Likewise, $t_0$ is the \textbf{start time}

\item[Number of offsets] The number of offsets is specified. If not \texttt{0}, then the time of each offset
is listed. The offset is modeled as a \textit{Heaviside} or unit step with the first value of the step corresponding
to the time specified of the offset.

\item[Number of exponential and/or log functions]  Many time series have exponential or logarithmic trends. This
option allows one to estimate the amplitude, and if desired, the time constant for each of these trends. If the number
is not \texttt{0}, then for each of the $k_{th}$ trends require:
\begin{description}
\item[Time of start of trend] Input the initial date of the trend, $t_T$, using format entered previously
\item[Time constant and either fix/float] This is two entries, the value of the time constant in \textbf{years} and
whether you want \textit{est\_noise} to estimate the time constant, \texttt{float} or hold the time constant \texttt{fix}'ed.
If \texttt{float} then the entered time constant is a guess and the program uses the Nelder and Mead (1965) algorithm
to find the optimal time constant assuming the current guess at the values of the data covariance.
\item[ Type of trend, exponential or logarithmic] Enter \texttt{e} for \textit{exponential}, $A e ^{-(t_i - t_T)/{\tau}}$, or
\texttt{m} for \textit{logarithmic} or \textit{Omori's law}, $A log (1 + {(t_i - t_T)/{\tau}})$
\end{description}

\item[Specify the date format of the data] The date format of the data may be different than the format of the dates
the specify the time dependence. See above for acceptable formats.

\item[Name of Data file] Provide the name of the data file. See above for acceptable formats.

\item[Specify then number of auxiliary data files] Usually, this will be \texttt{0}.  However, for some data sets,
such as borehole strainmeter data, this will be required as atmospheric pressure data
is used to adjust the raw strain data. One can enter other functions which might impact the observations.  The time
dependent function is set-up as:
\begin{equation}
d_i  = B + R (t_i - t_0) + ...  A_k (a_{k,i-j}) + ...
\end{equation}
where $A_k$ is the \textit{gain} or \textit{sensitivity} of the $k^{th}$ function (or data-channel) made up of $a_{k,i-j}$
$i$ \textit{observations} with a lag (in time) of $j$.  If the number of auxiliary data is not \texttt{0}, then the following entries
are required;
\begin{description}
\item[Format of data] Use one of the abbreviations listed under data formats.
\item[Name of Data file] Provide the name of the data file; Again, each record of data must have time,
the data value, and value for error, which is not used.
\item[Lead/Lag time] Usually \texttt{0} but can take on other values. It must be integer multiples
of the data sample interval given units of days.
\end{description}
The program will match the $a_{k,i-j}$ with the corresponding data, $d_i$. If one of the pair is missing, then
that line in the design matrix relating to the observation/data is deleted.

\item[Sampling interval] The nominal sampling interval, in days of the data.

\item[Type of error model] Acceptable values are \texttt{n}, \texttt{a}, \texttt{f}, or \texttt{c}.  See discussion in
the introduction, but usually this will either be \texttt{n}, or \texttt{a}. With \texttt{n}, the components of the noise
model are added in quadrature consistent with the legacy \textit{est\_noise6.50}. With the other three, filter functions of the noise model are simply added.  For data sets with few gaps or missing data, the program runs most efficiently
taking advantage of the algorithm discussed by \textit{Langbein} (2017) option \texttt{f} should be selected.
For gappy data, Cholesky decomposition is employed and option \texttt{c} should be selected.  By using
option \texttt{a}, the program automatically selects between the \textit{fast} or \textit{Cholesky} algorithms;
if there are more than 25\% missing data, the program defaults to Cholesky decomposition to invert the data
covariance matrix.

\item[Substitute synthetic noise for data]  This will be typically \texttt{n} or \textit{no}.  If \texttt{y}, the program will
ask a set of questions that specify the parameters of a noise model. Synthetic data are created with the same
sampling interval as the data, then at time when data are \textit{missing}, the synthetic data are deleted. \textbf{Note that
this option has not been tested with the current version of \textit{est\_noise7.sx} but had been used extensively in
the past. Use with caution!}  Instead, use the program \textit{gen\_noise7.02} to provide synthetic data.

\item[Decimation type]  Typically, this will be \texttt{0}, but one can put anything between \texttt{0} and \texttt{3}.
With \texttt{0}, all measurements are used.  Otherwise, the data will be decimated using the following, ad-hoc schedule:
\begin{description}
\item[1]  Keep two observations, delete the third, keep two, delete the third; uses $2/3$ of the available data.
\item[2] Keep two observations, delete the next two, keep two, delete two; uses $1/2$ of the available data.
\item[3] Keep two observations, delete the next three, keep two, delete three; uses $2/5$ of the available data.
\end{description}
If \textit{Type of error model} is anything but \texttt{n}, the \textit{decimation type} defaults to \texttt{0}, or no decimation.

\item[White noise amplitude] Set-up the parameters to estimate the white noise component. Two entries are required
with the first being a \textit{guess} at the value of the white noise and the second being \texttt{float} or \texttt{fix}.
If \texttt{float}, the program will find the optimal value of white noise that maximizes the log-likelihood. If \texttt{fix},
the program holds the value of white noise \textit{fixed}.  As an option, one can let \textit{est\_noise} make the guess
at the initial white noise value.  To do this, enter \texttt{-99999.0 float}.

\item[Amplitude of first power law noise model] Set-up the amplitude for the first power law/Gauss Markov noise model. Two entries are required
with the first being a \textit{guess} at the value of the amplitude of power law and the second being \texttt{float} or \texttt{fix}.
If \texttt{float}, the program will find the optimal value of the amplitude of power-law noise that maximizes the log-likelihood. If \texttt{fix},
the program holds the amplitude of power-law noise \textit{fixed}.  Note that the unit of the amplitude is $unit/{yr^{{\beta}/4}}$.
As an option, one can let \textit{est\_noise} make the guess
at the initial amplitude of power-law noise.  To do this, enter \texttt{-99999.0 float}.  The program will complete its guess
after the next parameter is entered as the power-law index is required.

\item[Index of first power law noise model] Continuation of the set-up of the first power-law/Gauss Markov noise model.
\textbf{Three} entries are required
with the first being a \textit{guess} at the of power law index and the second being \texttt{float} or \texttt{fix}.
If \texttt{float}, the program will find the optimal value of the index of power-law noise that maximizes the log-likelihood. If \texttt{fix},
the program holds the index of power-law noise \textit{fixed}.  If the index, $\beta=1$, then this is considered
as flicker noise and if $\beta=2$, then this is considered
as random-walk noise.  The last entry is places a limit on the size of the index during the optimization. The value
$3$ is a reasonable choice. See equation~1.

\item[Gauss Markov frequency] Continuation of the set-up of the first power-law/Gauss Markov noise model.
Two entries are required
with the first being a \textit{guess} at the ${\alpha}$ (radians/yr) and the second being \texttt{float} or \texttt{fix}.
If \texttt{float}, the program will find the optimal value G-M frequency that maximizes the log-likelihood. If \texttt{fix},
the program holds the G-M frequency \textit{fixed}.   See equation~1. In most cases, your entry will be \texttt{0 fix}.

\item[Bandpass filter limits] Pass-band for band-pass filtered noise model in units of cycles/year. Two numbers are
required. For typical seasonal noise, use \texttt{0.5 2.0}.  See equation~2.

\item[Number of poles for bandpass filtered noise] A integer value between \texttt{1} and \texttt{4} needs to be entered. See equation~2.

\item[Amplitude of bandpass filtered noise] Two entries are required
with the first being a \textit{guess} at the value of the amplitude of bandpass filtered noise and the second being \texttt{float} or \texttt{fix}.
If \texttt{float}, the program will find the optimal value of the amplitude  that maximizes the log-likelihood. If \texttt{fix},
the program holds the amplitude  \textit{fixed}.  If bandpass filtered noise is not considered, enter \texttt{0 fix}.

\item[Index of second power law noise model] Set-up of the second power-law noise model.
Two entries are required
with the first being a \textit{guess} for the of power law index and the second being \texttt{float} or \texttt{fix}.
If \texttt{float}, the program will find the optimal value of the index of power-law noise that maximizes the log-likelihood. If \texttt{fix},
the program holds the index of power-law noise \textit{fixed}.  If the index, $\beta=1$, then this is considered
as flicker noise and if $\beta=2$, then this is considered
as random-walk noise.

\item[Amplitude of second power law noise model] Continuation of the set-up the second power law noise model. Two entries are required
with the first being a \textit{guess} at the value of the amplitude of power law and the second being \texttt{float} or \texttt{fix}.
If \texttt{float}, the program will find the optimal value of the amplitude of power-law noise that maximizes the log-likelihood. If \texttt{fix},
the program holds the amplitude of power-law noise \textit{fixed}. Note that the unit of the amplitude is $unit/{yr^{{\beta}/4}}$.
Here, for the second power law entry, \textit{est\_noise} does \textbf{not} provide an option for the program to make
its guess at the power law amplitude.

\item[Add white noise to data] Usually, this will be \texttt{0}.  In some rare cases where the data are characterized
by extreme power-law noise with no detectable white noise, one may need to add white noise to the observations
to keep the inversion of the data covariance becoming numerically singular.

\end{description}

Although this is a long list of inputs, the program supplies descriptive queries.  The answers supplied to the queries
are preserved into a \textit{journal file} call \texttt{estin.jrn}. Note; one may need to delete extra lines following the
line specifying the name of the data file(s).

As suggested above, \textit{est\_noise} can provide a guess for both the white noise and the power-law amplitude. This
is useful if one has no idea of the scale of the error in the data.
To do this, the program computes two values of "root mean squared"; one that represents the high frequency component
and a second one for the complete data set.  The high frequency RMS is a measure of the white noise component while
the RMS for the entire data set is a measure of the combination of white noise plus colored noise components. The
white noise estimate, $\sigma_w$, is simply RMS $[d(i+1)-d(i)]/ \sqrt{2}$.  To compute the colored noise, one first computes
RMS of the data residuals after fitting a time-dependent model to the data using least-squares with a diagonal covariance matrix
and this is designated as $\sigma_t$.  The colored noise portion is the difference between the total and the white noise;
$\sigma_c = \sqrt { ( {\sigma_t}^2 - {\sigma_w}^2 )}$.  From a power spectrum of colored noise, one integrates over
frequency to obtain  ${\sigma_t}^2 = (1-n) P_c f ^ {1-n}$ evaluated at the nyquist frequency and the low-frequency limit.
Then, using equation~11 of \textit{Langbein} (2004), the power-law amplitude for the specified index of $n$ is estimated:
${\sigma_{pl}}^2 = (2*f_{ny})^{(1 - n/2)} P_c (2 \pi)^n / 2$, where $f_{ny}$ is the Nyquist frequency.  Finally, for the case
where the covariance matrix is \textit{additive} rather than \textit{quadrature}, the white noise estimate needs to be reduced by
$\sqrt {{\sigma_w}^2 - \sigma_w  \sigma_{pl} {\delta t}^{n/4} }$ where $ {\delta t}$ is the sampling interval.

\section{Output}

The program provides several outputs, with the standard out being the most informative. Often, especially
when running the program in batch mode, one should direct the standard output to a file. An example
is shown in the \texttt{./example} directory where \textit{est\_noise} was run as \\
\texttt{../bin/est\_noise7.30 < est\_canp.in > est\_canp.out} \\
In addition to the standard output, there are other files, \texttt{resid.out} and \texttt{covar.out} which augments the information
presented by the standard output. Specifically, each line in \texttt{resid.out} is the time, the difference
between the observed and predicted observation, the predicted value from the model, and the observation.
\texttt{covar.out} provides the covariance between the parameters of the time-dependent model and, secondly,
their cross correlations.  For \textit{trouble-shooting}, a file, \texttt{tauexp.out} is provided showing the estimates
of time-constants if the program is required to estimate these terms for either an exponential or logarithmic trend.

The standard output is divided roughly into four sections: Some statistics of the data including number of observations,
sampling interval, and number of gaps; The estimates
of the parameters of the time-dependent model \textit{assuming that the data are temporally independent, Gaussian
distributed}; A sequence of trials of noise parameters as the program iterates to an optimal value of log-likelihood;
And finally a section listing the estimates of the parameters of the time-dependent model and the accompanying
model for noise.

The various statistics compiled from the raw data include the number of points, the statistics of the data sampling,
the number of missing data, and an estimate of the white noise component made by taking the differences
between adjacent observations.

Initially, the program then assumes that the data covariance is a diagonal matrix and carries out the least-squares analysis
to estimate the size of each parameter and its standard error.  The standard error is rescaled based upon the
standard deviation of the fit of the model to the data, assuming a diagonal covariance matrix. (Presumably,
these would be the values you would get by plugging the data and the design matrix into spreadsheet).

The bulk of the standard output shows the values of each trial noise model and its corresponding likelihood. If
the time-constant of either an exponential or log-function is being estimated, it is shown, too.

Once the program maximizes the likelihood, the estimates of the parameters of time-dependent model 
and their standard errors are shown (they are recomputed at each iteration of noise modeling). 
The program attempts to estimate the uncertainty of the parameters of the noise model by
examining the curvature of the log-likelihood function during the optimization.  I don't believe these
results; they look too small.

Along with the value of the logarithm of the likelihood, MLE, both AIC and BIC, Akaike/Bayesian Information Criterion
which both factor in the total degrees of freedom from both the time-dependent and the noise modeling.  However,
I am skeptical of using  AIC/BIC as a method of choosing one noise model over another; More comments
can be found in the companion report called \textit{Threshold\_dMLE} found in this \textit{tar-ball}.

\section{Known problems}

The source program \texttt{time.f} is written by me a long time ago prior to the internet. I keep
updating the code to include the current, and future dates, but I think the current version is
limited to time between 1960 and 2040.

As noted above, the estimates of the standard errors for the parameters of the noise model are
probably too small.

The program has been extensively tested and run using the \texttt{otr} format. Recently, I did some
testing with the \texttt{gmt} format. I wouldn't be
surprised to find bugs with the other time-formats.

When using the \textit{f} or \textit{fast} option to create and invert the data covariance matrix and using
bandpass filtered noise as one component of the data-covariance, \textit{est\_noise7.3x} might "crash"
or provide non-sensible results. This is usually identified by seeing that there are a lot of NaN for some or
all of the trials of covariance parameters in the standard output. On the flip side, \textit{est\_noise7.3x} works
fine using the \textit{c} or \textit{cholesky} option, but for data sets with few gaps, the calculation is slower.
(The legacy option/quadrature \textit{n} works fine, too).  I believe that the problem of deconvolving a filter function
having a bandpass component is that the filter function is oscillatory with some points that are near zero. The inverse
filter function is one that, when convolved with the original filter, yields the \textit{delta} function. The algebra
required to solve this problem has division, and given that the original filter functions has terms that are close
to zero, their is a potential of obtaining large, oscillatory terms in the inverse function. Additional work, using
DFT rather than deconvolution, did not provide satisfactory results. My advise when bandpass filter noised is
desired, run  \textit{est\_noise7.3x} as usual, then, if the \textit{f} option is used and the results of the analysis either
doesn't make sense and/or there are NaN and Infinities in screen dump, then re-run with the \textit{c} option.


\section{Companion Programs}

As mentioned briefly in section 1.3, the \textit{est\_noise} package includes five other programs to
either generate temporally correlated noise, \textit{gen\_noise}, remove outliers, \textit{bust} in conjunction
with \textit{adjust}, and evaluate the results, \textit{compare\_wander} and \textit{psd\_calc}. All of these are
fortran programs that the user provides answers from a list of questions from each program. Since \textit{gen\_noise}
has a number of questions, a \textit{journal} file is created. With the exception of \textit{compare\_wander}, the documentation provided above for \textit{est\_noise}
should provide sufficient information to run each of these programs.

The input to  \textit{compare\_wander} requires two output files from \textit{est\_noise}, \texttt{resid.out} and \texttt{max.dat}.
The \texttt{resid.out} lists the misfits to the time-dependent model prescribed for \textit{est\_noise}, while \texttt{max.dat}
provides the estimated parameters of the noise model.  The output of \textit{compare\_wander} is found in \texttt{wander.out}
which provides a measure of the drift; this is characterized by computing the RMS of $(x(t+{\tau})-x(t))$ as a function of various
intervals, ${\tau}$, for  time series (\textit{Agnew} 1992). The quality of the noise model provided by \textit{est\_noise} can be assessed through
simulating data using the noise model from \textit{est\_noise} and computing the drift from each set of simulated data.
I recommend that at least 200 simulations.  The drift from each simulation is stored, then, for each ${\tau}$ interval,
the RMS drift is sorted and then tabulated to establish the median drift along with the 68\% and 95\% confidence levels.
These statistics, along with the drift of the original data should be plotted for a visual assessment of the quality of the
modeled noise and time dependence.


\section{Sample scripts}

In the directory \texttt{example0}, I provide five scripts to analyze two different time series. Each script should be
sufficiently documented such that they could be reused for other applications. Documentation is obtained by
typing the name of the script. However, to see the results,
they do need the GMT plotting library; \textit{www.soest.hawaii.edu/gmt/}. These scripts however, use an older version
of GMT, which is version 4. (\textit{ftp://ftp.soest.hawaii.edu/gmt/legacy})

The five scripts are:
\begin{description}
\item[cleanEst.sh] This should be used first as it 1) makes a preliminary estimate of the parameters of the time dependence of the 
data using a default, \textit{canned} noise model that is roughly appropriate for continuous GNSS data; 2) detrends those data, 3) removes the outliers, and 4) reinserts the trends. The residuals to the fit are plotted so that one can
add additional time-dependent models as required or remove outliers not caught by \textit{bust\_5}.   In addition, the input to successive calls to \textit{est\_noise} is set-up, which are required in the next four scripts. The work performed by this and the other scripts is
found in \texttt{/tmp/SCRATCH/\textit{data\_file\_name}}.
\item[EstNoise.sh]  Uses the cleaned data, \texttt{data.cl} and the file \texttt{est0.in} from \textit{cleanEst.sh} along with a user selected noise
model and runs \textit{est\_noise} to find the optimal parameters of the noise model.  The residuals to this fit are plotted.
In addition, the output of \textit{est\_noise} is feed into \textit{compare\_wander} and the RMS drift of the data, along with
the statistics from a set from simulations are plotted.  Finally, the equivalent power spectral density is plotted.
\item[EstNoiseAll.sh]  Instead of evaluating a single type of noise model, this script evaluates a sequence of noise models
as a batch process such that the best noise model can be identified after evaluating the changes in Log(likelihood) and
the corresponding drift.  Initially, both the random-walk plus white noise and the flicker plus white noise models are
computed.  The script picks the one with the \textit{maximum likelihood} and identifies it as the so called \textit{null} model.
Next, the power law model is evaluated and its likelihood is compared with that from the \textit{null} model. Similarly,
the flicker plus random-walk noise is evaluated. Of these four, it is likely that one of these is a clear "winner". However,
the script goes on to evaluate both first-order Gauss-Markov and generalized G-M noise. A final set of evaluations uses
bandpass filtered noise superimposed on either flicker or random-walk noise, and flicker plus random walk noise (the default) or power-law noise with the \texttt{-P} option.  Using the \texttt{-S} option,
one can limit the number of noise models tested from the original eight.  Use the \texttt{-B} to change the limits
of the band-passed filter. Using \texttt{-m} changes the limit on the power-law index from the default value of $3$.For all of these, a drift curve is evaluated. More description of the script can be obtained with \texttt{-h}.
\end{description}

Two additional scripts are provided that extend the analysis of \textit{EstNoiseAll.sh}.  These scripts allow for the combination of band-passed
filtered noise and generalized Gauss-Markov noise. \textit{EstNoiseAll.sh} allows for either GGM or band-passed filtered noise. Note that \textit{EstNoiseAll.sh} 
needs to be run prior to either \textit{EstNoiseMore.sh} or \textit{EstNoiseAdded.sh}.
\begin{description}
\item[EstNoiseMore.sh]  This script tests all four possibilities of band-passed filtered noise (four different poles) along with estimating the Gauss-Markov frequency (radians/yr), power-law index and amplitude.
\item[EstNoiseAdded.sh] This script tests only one band-passed filtered noise by selecting the pole found by running textit{EstNoiseAll.sh} that
it identified as optimal.
\end{description}
These scripts currently set for the \texttt{otr} format and mostly set-up for
the \texttt{gmt} format. 

Prior to running each of these scripts, you will need to edit each script to identify where the executable programs reside.
This line should be located near the top of the file containing the script with an environment variable \texttt{progs}.  The actual
work by these scripts is carried out in \texttt{/tmp/SCRATCH/\textit{data\_file\_name}}.

By typing the command, one should get documentation with regards to the inputs to these files. The \texttt{example0} directory
contains the scripts, two data sets, files that are required for the inputs, and the output plot files.

With \textit{cleanEst.sh} there are three options that require additional files.  These options allow for removing bad data, \texttt{-E}.
estimating the size of offsets, \texttt{-E}. and estimating additional trends in the  \texttt{-T}.  For each of these, they require specific formats
and there are examples shown in the \texttt{example0} directory (\texttt{canp.edit, canp.off and canp.trnd}.

\begin{description}
\item[-E] Deleting observations;
\linebreak \texttt{cat canp.edit
2004 272 2004 274}
\item[-O] Offset;
\linebreak \texttt{cat canp.off
\linebreak 
2003 356
\linebreak
2004 272}
\item[-T] Trend;
\texttt{
\linebreak
cat canp.trd 
\linebreak 
m 2003 356 0.0054 fix}. \scriptsize{Omori function with 0.0054 year time delay held constant}
\normalsize
\linebreak
\texttt{ r 2000 1 2007 120}  \scriptsize{Change in rate}
\normalsize
\linebreak
\texttt{m 2004 272 0.0054 float} \scriptsize{Omori function with 0.0054 year time delay as initial guess}

\end{description}

\subsection{Example 1}

The data are collected from a site with observations spanning both the San Simeon and Parkfield  Earthquakes (2003 and 2004
respectively). The time dependent model includes sinusoids, a secular rate, offsets due to both earthquakes, \texttt{canp.off},
two Omori law, decay functions and a rate change, \texttt{canp.trd}, and a few observations deleted just after the Parkfield
earthquake, \texttt{canp.edit}.  The initial "cleaning" is obtained by:
\\
\texttt{cleanEst.sh -d canp.e -f otr -O canp.off -T canp.trd -E canp.edit}
\\

Because the time constant for the Omori function for the Parkfield earthquake is being estimated, \textit{est\_noise} runs
slower than normal.  The results can be found in \texttt{canp.e.ps} and \texttt{est\_canp.e.out}.  To speed-up the
next set of evaluations, I have modified the \texttt{.trd} file by \textbf{fixing} the Omori time constant for the Parkfield
Earthquake to $0.0061746$ years which is shown in \texttt{canp1.trd}. Consequently, I re-run \textit{cleanEst.sh}
\\
\texttt{cleanEst.sh -d canp.e -f otr -O canp.off -T canp1.trd -E canp.edit}
\\

To evaluate a specific noise model, I run \textit{EstNoise.sh} as:
\\
\texttt{EstNoise.sh -d canp.e  -M PL}
\\
The outputs of this analysis are shown in  \texttt{canp.e\_PL.ps} and \texttt{est\_canp.e\_PL.out}.  The plot shows the
residuals to a fit to a time dependent model, the drift of the data compared to simulated data having the same
model of noise, and the equivalent power spectra of the noise model.  The drift suggests that observed data has slightly
more noise over the 300 to 1000 day periods that the noise provided by a power-law noise model.  Consequently, I could try a combination of flicker and random walk.
\\
\texttt{EstNoise.sh -d canp.e  -M FLRW}
\\
Indeed, examination of the drift, \texttt{canp.e\_FLRW.ps} suggests an improvement in the fit. In addition, the increase
in Log likelihood, from -7667.7 to -7660.9 from the PL to the FLRW model suggests that the flicker and random-walk 
model of noise is more appropriate for this data set.
Likewise, other noise models can be evaluated. 

However, a more comprehensive search for the optimal noise model is carried out with \textit{EstNoiseAll.sh}. For this example,
it is invoked with:
\\
\texttt{EstNoiseAll.sh -d canp.e }
\\
Examination of the results of the drift spectra along with the values of Log(likelihood) (MLE), AIC, and BIC shown in
\texttt{canp.e\_all.ps} suggests that the FLRW noise model is best since it has the largest MLE and the smallest AIC and BIC.

\subsection{Example 2}

This is an example of simulated noise having first order Gauss Markov noise. In addition, there is an offset, and the
data use the \textit{gmt} format.  Initially, the data are "cleaned";
\\
\texttt{cleanEst.sh -d noise2.gmt -f gmt -E noise.edit -O noise.off}
\\
The results are presented in \texttt{noise2.gmt.ps} and \texttt{est\_noise2.gmt.out}.
\\

Noise modeling is accomplished by:
\\
\texttt{EstNoise.sh  -d noise2.gmt  -M RW}
\\
Although other noise models can be evaluated, PL, FOGM, and GM, none seem to be much better than the random walk
model.

Likewise, a search of optimal noise models is provided by invoking:
\\
\texttt{EstNoiseAll.sh  -d noise2.gmt}
\\
Examination of the results, \texttt{noise2.gmt\_all.ps} suggests that RW is probably the best noise model since
its closest competitor, FOGM, has marginally larger MLE (1.39) and lower AIC (-0.8) relative to the null model of RW.
On the other hand, BIC for FOGM is worse than for RW.  The most conservative choice would be to chose the model
having random-walk represent the data at its longest periods which provides larger standard error in rate relative to
the FOGM model.

\section{Updates}

Updates are included in a file call \texttt{BugFix.txt}.

\section{References}

Agnew, D. (1992), The time domain behavior of power law noises, \textit{Geophys. Res. Lett.},  doi:{10.1029/91GL02832}.

Bos, M.~S., R.~M.~S.~ Fernandes, S.~P.~D.~Williams, and L.~Bastos (2012), Fast error analysis of
continuous GPS observations,
\textit{J. Geod.},  doi:{10.1007/s00190-007-0165-x}.

%Hackl, M., R.~Malservisi, U.~Hugentobler, and R.~Wonnacott (2011), Estimation of velocity uncertainties
%from GPS time series: examples from analysis of the South African TrigNet network, \textit{J.  Geophy. Res.},
 %\textit{116B15:11404}
 
Langbein, J., and H. Johnson (1997), Correlated error in geodetic time series: Implications for
time-dependent deformation, \textit{J.  Geophy. Res.},  \textit{102}, 591--604.

Langbein, J. (2004), Noise in two-color electronic distance meter measurements revisited, 
 \textit{J.  Geophy. Res.}  doi:{10.1029/ 2003JB002819}.
 
Langbein, J. [2008], Noise in GPS displacement measurements from Southern California and Southern Nevada, 
 \textit{J.  Geophy. Res.}  doi{10.1029/2007JB005247}.

Langbein, J. [2009], Computer algorithm for analyzing and processing bore- hole strainmeter data, \textit{Comput. Geosci.}, 36(5), 611-619, doi:{10.1016/j.cageo.2009.08.011}

Langbein, J. [2012], Estimating rate uncertainty with maximum likelihood: differences between power-law and flicker/random-walk 
models, \textit{J. Geod.} doi:{10.1007/s00190-012-0556-5}

Langbein, J. [2017], Improved method for maximum likelihood analysis of time series with temporally correlated errors,
\textit{J. Geod.} doi{0.1007/s00190-017-1002-5}.  note that a copy of this paper is included in the documentation

Nelder, J. A., and R. Mead (1965), A simplex method for function minimization, \textit{Computer Journal 7:} 308?313. doi:10.1093/comjnl/7.4.308

Press,, W.H, S.A. Teukolsky, W.T. Vetterling, and B.P. Flannery, (1992) \textit{Numerical Recipes in Fortran 77}, second edition,
Cambridge University Press.

Williams, S. D. P., Y. Bock, P. Fang, P. Jamason, R. M. Nikolaidis,
L. Prawirodirdjo, M. Miller, and D. J. Johnson (2004), Error analysis of continuous GPS position time series, 
 \textit{J.  Geophy. Res.}  doi:{10.1029/2003JB002741}.




\end{document}  